{"createdAt":"2025-06-30T04:38:36.302Z","updatedAt":"2025-06-30T11:57:21.000Z","id":"J6RvMgomFJG1hqvN","name":"Token_Estim8r","active":true,"isArchived":false,"nodes":[{"parameters":{},"type":"n8n-nodes-base.manualTrigger","typeVersion":1,"position":[780,1020],"id":"271d1c01-45e4-4ebe-91c5-ac7ec1cef8d5","name":"When clicking ‘Test workflow’"},{"parameters":{"content":"# Token Estim8r（トークン見積もりツール）\n\n# セットアップ チェックリスト\n\n### ✅ Token Estim8r セットアップ チェックリスト\n\n- CSVからGoogleスプレッドシートを作成\n- Get AI PricingノードでJina APIの認証情報を設定\n- 正しいGoogleスプレッドシートを選択\n- Token Estim8rにデータを送信するHTTPリクエストノードを追加\n- 実行してスプレッドシートを確認！ 🤩\n\n# セットアップ手順ガイド\n\n### **1. Googleスプレッドシートを準備する**\n\n- 以下のCSVテンプレートのテキストをコピーします。\n- テキストファイルに貼り付けます。\n- 拡張子を.csvにしてファイルを保存します。\n\n### 📋 CSVテンプレート\n\n### **これをお好みのテキストエディタにコピーして「.csv」ファイルとして保存し、そのCSVファイルをGoogleスプレッドシートにインポートしてください。**\n\nGenerated csv\n\n`timestamp, Workflow Name, Workflow ID, Workflow Execution IdTotal Tokens, Prompt Tokens, Completion Tokens, Models Used, Tools Used, Total Cost, Json Array`\n\n**content_copydownload**Use code [**with caution**](https://support.google.com/legal/answer/13505487).Csv\n\n---\n\n### **2. ライブ価格を設定する（ライブ価格機能を使用する場合のみ）**\n\n- 「Token Estim8r」ワークフローの中からGet AI Pricingノードを見つけます。\n- リアルタイムで価格を取得できるよう、ノードにJina APIの認証ヘッダーを設定します。\n\n### **3. Googleスプレッドシートを接続する**\n\n- 「Token Estim8r」ワークフローで、先ほどCSVファイルからインポートしたGoogleスプレッドシートを選択します。\n\n### **4. 実行データを送信するためのHTTPリクエストを追加する**\n\n- トークン使用量を記録したいワークフローの最後に、HTTP Requestノードを追加します。\n- メソッドをPOSTに設定し、送信先URLに「Token Estim8r」ワークフローのWebhook URLを指定します。\n- JSONボディには、 { \"executionId\": \"{{$execution.id}}\" } を設定して送信します。\n\n### **5. 実行して確認する**\n\n- ワークフローを実行します。\n- Googleスプレッドシートを開き、トークン使用量とコストのデータが正しく記録されていることを確認します。 🎉\n\n---\n\nこれでセットアップは完了です。使用状況の監視とコスト計算を始めましょう！","height":1300,"width":700},"type":"n8n-nodes-base.stickyNote","position":[0,400],"typeVersion":1,"id":"53b8ea44-b25f-42eb-aa4c-4ccbb7367dd5","name":"Sticky Note"},{"parameters":{"content":"## Created by **[Real Simple Solutions](https://realsimple.dev/?utm_source=workflow&utm_medium=template&utm_campaign=tokenestim8r)** | More ⏰ Time Saving Templates 👉 **[Click Here](https://joeperes.gumroad.com/?utm_source=workflow&utm_medium=template&utm_campaign=tokenestim8r)**","height":80,"width":960,"color":5},"type":"n8n-nodes-base.stickyNote","position":[1380,1000],"typeVersion":1,"id":"5de0cf6e-17f2-4079-a577-ddf83dc3dc6e","name":"Sticky Note6"},{"parameters":{"amount":10},"type":"n8n-nodes-base.wait","typeVersion":1.1,"position":[1140,740],"id":"18a81c09-8047-4206-b181-d5da926c3799","name":"Wait","webhookId":"3ac46353-7e05-4670-a86e-d2acdd1dc9e4"},{"parameters":{"httpMethod":"POST","path":"token-estim8r-data","options":{}},"type":"n8n-nodes-base.webhook","typeVersion":2,"position":[780,740],"id":"de2db38d-332a-4980-9b33-768a64733736","name":"Webhook2","webhookId":"aa3da05f-b451-46cb-bc49-7801bd545e0a"},{"parameters":{"url":"={{ $execution.resumeUrl.split('/').slice(0, 3).join('/') }}/api/v1/executions/{{ $('Webhook2').item.json.body.executionId }}?includeData=true","authentication":"predefinedCredentialType","nodeCredentialType":"n8nApi","options":{}},"type":"n8n-nodes-base.httpRequest","typeVersion":4.2,"position":[1440,740],"id":"37f2a423-d4d7-4fbd-9549-0af5138bcc67","name":"n8n - Get Execution","credentials":{"n8nApi":{"id":"AUyHyAgWNH65WOWn","name":"n8n account"}}},{"parameters":{"jsCode":"// Initialize a tokens accumulator\nconst tokens = {\n  completionTokens: 0,\n  promptTokens: 0,\n  totalTokens: 0,\n};\n\n// Debug mode - set to true to see detailed logs\nconst DEBUG = true;\n\n// Store individual token usages for inspection\nlet tokenUsageSources = [];\n\n// Track usage by model name\nconst modelUsage = {};\n\n// Track usage by tool/node name\nconst toolUsage = {};\n\n// Track all unique model names found\nconst modelsUsed = new Set();\n\n// Track all unique tool names found\nconst toolsUsed = new Set();\n\n// Model name mapping for common LLM providers (Updated for June 2025)\nconst MODEL_NAME_MAPPING = {\n  // OpenAI models - Updated with latest models\n  'gpt-4.5': 'gpt-4.5',\n  'gpt-4.1': 'gpt-4.1',\n  'gpt-4': 'gpt-4',\n  'gpt-4-turbo': 'gpt-4-turbo',\n  'gpt-4o': 'gpt-4o',\n  'gpt-4o-mini': 'gpt-4o-mini',\n  'gpt-4-vision': 'gpt-4-vision',\n  'gpt-3.5': 'gpt-3.5-turbo',\n  'gpt-3.5-turbo': 'gpt-3.5-turbo',\n  'o3-pro': 'o3-pro',\n  'o3': 'o3',\n  'o4-mini': 'o4-mini',\n  'text-davinci': 'text-davinci',\n  \n  // Anthropic models\n  'claude': 'claude',\n  'claude-2': 'claude-2',\n  'claude-instant': 'claude-instant',\n  'claude-3': 'claude-3',\n  'claude-3-opus': 'claude-3-opus',\n  'claude-3-sonnet': 'claude-3-sonnet',\n  'claude-3-haiku': 'claude-3-haiku',\n  'claude-3.5': 'claude-3.5',\n  'claude-3.5-opus': 'claude-3.5-opus',\n  'claude-3.5-sonnet': 'claude-3.5-sonnet',\n  'claude-3.5-haiku': 'claude-3.5-haiku',\n  'claude-4': 'claude-4',\n  'claude-opus-4': 'claude-opus-4',\n  'claude-sonnet-4': 'claude-sonnet-4',\n  \n  // Google models\n  'gemini': 'gemini',\n  'gemini-pro': 'gemini-pro',\n  'gemini-ultra': 'gemini-ultra',\n  'gemini-1.5-pro': 'gemini-1.5-pro',\n  'gemini-1.5-flash': 'gemini-1.5-flash',\n  'gemini-2.0-flash': 'gemini-2.0-flash',\n  'gemini-2.0-pro': 'gemini-2.0-pro',\n  'gemini-2.5-pro': 'gemini-2.5-pro',\n  'gemini-2.5-flash': 'gemini-2.5-flash',\n  'gemini-2.5-flash-lite': 'gemini-2.5-flash-lite',\n  \n  // DeepSeek models\n  'deepseek': 'deepseek-chat',\n  'deepseek-chat': 'deepseek-chat',\n  'deepseek-coder': 'deepseek-coder',\n  'deepseek-r1': 'deepseek-r1',\n  'deepseek-v3': 'deepseek-v3',\n  'deepseek-v2': 'deepseek-v2',\n  \n  // Mistral models\n  'mistral': 'mistral',\n  'mistral-small': 'mistral-small',\n  'mistral-medium': 'mistral-medium',\n  'mistral-medium-3': 'mistral-medium-3',\n  'mistral-large': 'mistral-large',\n  \n  // Meta models\n  'llama': 'llama',\n  'llama-2': 'llama-2',\n  'llama-3': 'llama-3',\n  'llama-3.3': 'llama-3.3-70b',\n  'llama-4': 'llama-4'\n};\n\n// Helper function to normalize and identify model names\nfunction normalizeModelName(modelNameInput) {\n  if (!modelNameInput || typeof modelNameInput !== 'string') {\n    return \"Unknown\";\n  }\n  \n  const modelName = modelNameInput.toLowerCase();\n  \n  // Handle specific OpenAI model patterns first\n  if (modelName.includes('gpt-4o-mini')) {\n    return 'gpt-4o-mini';\n  }\n  if (modelName.includes('gpt-4o')) {\n    return 'gpt-4o';\n  }\n  if (modelName.includes('gpt-4.5')) {\n    return 'gpt-4.5';\n  }\n  if (modelName.includes('gpt-4.1')) {\n    return 'gpt-4.1';\n  }\n  if (modelName.includes('gpt-4')) {\n    return 'gpt-4';\n  }\n  if (modelName.includes('gpt-3.5') || modelName.includes('gpt3.5')) {\n    return 'gpt-3.5-turbo';\n  }\n  if (modelName.includes('o4-mini')) {\n    return 'o4-mini';\n  }\n  if (modelName.includes('o3-pro')) {\n    return 'o3-pro';\n  }\n  if (modelName.includes('o3')) {\n    return 'o3';\n  }\n  \n  // Direct match in our mapping\n  for (const [key, value] of Object.entries(MODEL_NAME_MAPPING)) {\n    if (modelName.includes(key)) {\n      return value;\n    }\n  }\n  \n  // Return the original if we can't normalize it\n  return modelNameInput;\n}\n\n// Function to extract model name from various sources\nfunction extractModelName(obj, path, nodeName = '') {\n  let modelName = \"Unknown\";\n  \n  if (DEBUG) console.log(`Extracting model from path: ${path}, nodeName: ${nodeName}`);\n  \n  // 1. Check for model in json.model field (OpenAI direct API responses)\n  if (obj.json && obj.json.model) {\n    if (DEBUG) console.log(`Found model in json.model: ${obj.json.model}`);\n    return normalizeModelName(obj.json.model);\n  }\n  \n  // 2. Check for model in generationInfo.model_name (LangChain responses)\n  if (obj.json && \n      obj.json.response && \n      obj.json.response.generations && \n      Array.isArray(obj.json.response.generations) && \n      obj.json.response.generations[0] && \n      Array.isArray(obj.json.response.generations[0]) && \n      obj.json.response.generations[0][0] && \n      obj.json.response.generations[0][0].generationInfo) {\n    \n    const generationInfo = obj.json.response.generations[0][0].generationInfo;\n    if (generationInfo.model_name || generationInfo.model) {\n      if (DEBUG) console.log(`Found model in generationInfo: ${generationInfo.model_name || generationInfo.model}`);\n      return normalizeModelName(generationInfo.model_name || generationInfo.model);\n    }\n  }\n  \n  // 3. Check for model in inputOverride (LangChain node configurations)\n  if (obj.inputOverride && \n      obj.inputOverride.ai_languageModel && \n      Array.isArray(obj.inputOverride.ai_languageModel) && \n      obj.inputOverride.ai_languageModel[0] && \n      Array.isArray(obj.inputOverride.ai_languageModel[0]) && \n      obj.inputOverride.ai_languageModel[0][0] && \n      obj.inputOverride.ai_languageModel[0][0].json && \n      obj.inputOverride.ai_languageModel[0][0].json.options) {\n    \n    const options = obj.inputOverride.ai_languageModel[0][0].json.options;\n    if (options.model || options.model_name) {\n      const modelFound = options.model || options.model_name;\n      if (DEBUG) console.log(`Found model in inputOverride: ${modelFound}`);\n      return normalizeModelName(modelFound);\n    }\n  }\n  \n  // 4. Extract from node name in path\n  if (nodeName) {\n    if (DEBUG) console.log(`Extracting model from node name: ${nodeName}`);\n    \n    if (nodeName.includes('OpenAI')) {\n      // For direct OpenAI nodes, we should have found the model in json.model\n      // If not, default to a common OpenAI model\n      return 'gpt-4o-mini'; // Default for OpenAI nodes\n    } else if (nodeName.includes('Gemini') || nodeName.includes('Google')) {\n      return 'gemini-1.5-pro';\n    } else if (nodeName.includes('Claude') || nodeName.includes('Anthropic')) {\n      return 'claude-3.5-sonnet';\n    } else if (nodeName.includes('Groq')) {\n      return 'llama-3';\n    } else if (nodeName.includes('DeepSeek')) {\n      return 'deepseek-chat';\n    } else if (nodeName.includes('Mistral')) {\n      return 'mistral';\n    }\n  }\n  \n  return modelName;\n}\n\n// List of nodes to exclude from token counting (flow control nodes that pass through data)\nconst EXCLUDED_NODES = [\n  'If',\n  'Switch', \n  'Merge',\n  'Wait',\n  'Stop and Error',\n  'No Operation',\n  'Set',\n  'Edit Fields',\n  'Filter'\n];\n\n// Function to extract node name from path\nfunction extractNodeNameFromPath(path) {\n  if (!path) return '';\n  \n  const pathParts = path.split('.');\n  for (const part of pathParts) {\n    // Look for runData entries which contain node names\n    if (part.includes('runData') && pathParts.indexOf(part) < pathParts.length - 1) {\n      const nextPart = pathParts[pathParts.indexOf(part) + 1];\n      // Extract node name (remove array indices)\n      return nextPart.split('[')[0];\n    }\n  }\n  \n  // Fallback: look for node-like names in the path\n  for (const part of pathParts) {\n    const nodeName = part.split('[')[0];\n    if (nodeName.includes('OpenAI') || \n        nodeName.includes('Claude') || \n        nodeName.includes('Gemini') || \n        nodeName.includes('Model') ||\n        nodeName.includes('AI')) {\n      return nodeName;\n    }\n  }\n  \n  return '';\n}\n\n// Function to check if a node should be excluded from token counting\nfunction shouldExcludeNode(nodeName) {\n  if (!nodeName) return false;\n  \n  // Check exact matches\n  if (EXCLUDED_NODES.includes(nodeName)) {\n    return true;\n  }\n  \n  // Check partial matches for common flow control patterns\n  const lowerNodeName = nodeName.toLowerCase();\n  const excludePatterns = [\n    'if',\n    'switch',\n    'merge',\n    'wait',\n    'filter',\n    'set',\n    'edit'\n  ];\n  \n  return excludePatterns.some(pattern => lowerNodeName.includes(pattern));\n}\n\n// Function to check if an object contains token usage information\nfunction isTokenUsageObject(obj) {\n  if (!obj || !obj.json) return false;\n  \n  const json = obj.json;\n  \n  // Check for different token usage formats\n  return (\n    // OpenAI direct API format: json.usage with snake_case\n    (json.usage && \n     typeof json.usage === 'object' && \n     (json.usage.total_tokens !== undefined || \n      json.usage.prompt_tokens !== undefined || \n      json.usage.completion_tokens !== undefined)) ||\n    \n    // LangChain format: json.tokenUsage with camelCase\n    (json.tokenUsage && \n     typeof json.tokenUsage === 'object' && \n     (json.tokenUsage.totalTokens !== undefined || \n      json.tokenUsage.promptTokens !== undefined || \n      json.tokenUsage.completionTokens !== undefined)) ||\n    \n    // Alternative format: json.tokenUsageEstimate\n    (json.tokenUsageEstimate && \n     typeof json.tokenUsageEstimate === 'object' && \n     (json.tokenUsageEstimate.totalTokens !== undefined || \n      json.tokenUsageEstimate.promptTokens !== undefined || \n      json.tokenUsageEstimate.completionTokens !== undefined))\n  );\n}\n\n// Function to extract token usage from an object\nfunction extractTokenUsage(obj) {\n  const json = obj.json;\n  let completionTokens = 0;\n  let promptTokens = 0;\n  let totalTokens = 0;\n  \n  // Priority order: usage (OpenAI direct) > tokenUsage (LangChain) > tokenUsageEstimate\n  if (json.usage) {\n    completionTokens = json.usage.completion_tokens || json.usage.completionTokens || 0;\n    promptTokens = json.usage.prompt_tokens || json.usage.promptTokens || 0;\n    totalTokens = json.usage.total_tokens || json.usage.totalTokens || 0;\n  } else if (json.tokenUsage) {\n    completionTokens = json.tokenUsage.completionTokens || 0;\n    promptTokens = json.tokenUsage.promptTokens || 0;\n    totalTokens = json.tokenUsage.totalTokens || 0;\n  } else if (json.tokenUsageEstimate) {\n    completionTokens = json.tokenUsageEstimate.completionTokens || 0;\n    promptTokens = json.tokenUsageEstimate.promptTokens || 0;\n    totalTokens = json.tokenUsageEstimate.totalTokens || 0;\n  }\n  \n  return { completionTokens, promptTokens, totalTokens };\n}\n\n// Function to extract finish reason from the object\nfunction extractFinishReason(obj) {\n  const json = obj.json;\n  \n  // OpenAI direct API format\n  if (json.choices && Array.isArray(json.choices) && json.choices[0]) {\n    return json.choices[0].finish_reason;\n  }\n  \n  // LangChain format\n  if (json.response && \n      json.response.generations && \n      Array.isArray(json.response.generations) && \n      json.response.generations[0] && \n      Array.isArray(json.response.generations[0]) && \n      json.response.generations[0][0] && \n      json.response.generations[0][0].generationInfo) {\n    \n    const generationInfo = json.response.generations[0][0].generationInfo;\n    return generationInfo.finishReason || generationInfo.finish_reason || null;\n  }\n  \n  return null;\n}\n\n// Recursive function to find all token usage objects\nfunction findTokenUsage(obj, path = \"root\") {\n  let results = [];\n  \n  if (obj === null || obj === undefined) {\n    return results;\n  }\n  \n  // Check if this object contains token usage\n  if (typeof obj === 'object' && isTokenUsageObject(obj)) {\n    const nodeName = extractNodeNameFromPath(path);\n    \n    // Skip if this is from an excluded node\n    if (shouldExcludeNode(nodeName)) {\n      if (DEBUG) {\n        console.log(`\\n=== EXCLUDED Token Usage ===`);\n        console.log(`Path: ${path}`);\n        console.log(`Node: ${nodeName} (excluded from counting)`);\n      }\n      // Continue searching but don't count this token usage\n    } else {\n      const modelName = extractModelName(obj, path, nodeName);\n      const tokenUsage = extractTokenUsage(obj);\n      const finishReason = extractFinishReason(obj);\n      \n      if (DEBUG) {\n        console.log(`\\n=== Found Token Usage ===`);\n        console.log(`Path: ${path}`);\n        console.log(`Node: ${nodeName}`);\n        console.log(`Model: ${modelName}`);\n        console.log(`Tokens: ${JSON.stringify(tokenUsage)}`);\n        console.log(`Finish Reason: ${finishReason}`);\n      }\n      \n      const usage = {\n        path,\n        node: nodeName || 'Unknown Node',\n        model: modelName,\n        finishReason,\n        ...tokenUsage\n      };\n      \n      results.push(usage);\n    }\n  }\n  \n  // Recursively search through object properties\n  if (typeof obj === 'object') {\n    if (Array.isArray(obj)) {\n      // Handle arrays\n      for (let i = 0; i < obj.length; i++) {\n        const newPath = `${path}[${i}]`;\n        const childResults = findTokenUsage(obj[i], newPath);\n        results = results.concat(childResults);\n      }\n    } else {\n      // Handle objects\n      for (const key in obj) {\n        if (obj.hasOwnProperty(key)) {\n          const newPath = `${path}.${key}`;\n          const childResults = findTokenUsage(obj[key], newPath);\n          results = results.concat(childResults);\n        }\n      }\n    }\n  }\n  \n  return results;\n}\n\n// Get the input data\nlet rawData;\nif (typeof $input !== 'undefined') {\n  // We're in n8n environment\n  rawData = $input.first()?.json;\n} else {\n  throw new Error(\"No input data found.\");\n}\n\nif (!rawData) {\n  throw new Error(\"No input data found.\");\n}\n\n// Start the recursive search from the root\nif (DEBUG) console.log(\"Starting token usage search...\");\ntokenUsageSources = findTokenUsage(rawData, 'root');\n\nif (DEBUG) console.log(`\\nFound ${tokenUsageSources.length} token usage sources`);\n\n// Process each token usage source\ntokenUsageSources.forEach((tokenUsage, index) => {\n  if (DEBUG) {\n    console.log(`\\n--- Processing Token Usage ${index + 1} ---`);\n    console.log(`Node: ${tokenUsage.node}`);\n    console.log(`Model: ${tokenUsage.model}`);\n    console.log(`Tokens: Completion=${tokenUsage.completionTokens}, Prompt=${tokenUsage.promptTokens}, Total=${tokenUsage.totalTokens}`);\n  }\n  \n  // Add to the unique models set\n  if (tokenUsage.model !== \"Unknown\") {\n    modelsUsed.add(tokenUsage.model);\n  }\n  \n  // Add to the unique tools set\n  if (tokenUsage.node !== \"Unknown Node\") {\n    toolsUsed.add(tokenUsage.node);\n  }\n  \n  // Update model usage tracking\n  if (!modelUsage[tokenUsage.model]) {\n    modelUsage[tokenUsage.model] = {\n      completionTokens: 0,\n      promptTokens: 0,\n      totalTokens: 0,\n      count: 0,\n      finishReasons: {},\n      nodes: {}\n    };\n  }\n  \n  modelUsage[tokenUsage.model].completionTokens += tokenUsage.completionTokens || 0;\n  modelUsage[tokenUsage.model].promptTokens += tokenUsage.promptTokens || 0;\n  modelUsage[tokenUsage.model].totalTokens += tokenUsage.totalTokens || 0;\n  modelUsage[tokenUsage.model].count += 1;\n  \n  // Track finish reasons\n  if (tokenUsage.finishReason) {\n    modelUsage[tokenUsage.model].finishReasons[tokenUsage.finishReason] = \n      (modelUsage[tokenUsage.model].finishReasons[tokenUsage.finishReason] || 0) + 1;\n  }\n  \n  // Track nodes used with this model\n  if (!modelUsage[tokenUsage.model].nodes[tokenUsage.node]) {\n    modelUsage[tokenUsage.model].nodes[tokenUsage.node] = {\n      count: 0,\n      totalTokens: 0\n    };\n  }\n  modelUsage[tokenUsage.model].nodes[tokenUsage.node].count += 1;\n  modelUsage[tokenUsage.model].nodes[tokenUsage.node].totalTokens += tokenUsage.totalTokens || 0;\n  \n  // Update tool/node usage tracking\n  if (!toolUsage[tokenUsage.node]) {\n    toolUsage[tokenUsage.node] = {\n      completionTokens: 0,\n      promptTokens: 0,\n      totalTokens: 0,\n      count: 0,\n      models: {}\n    };\n  }\n  \n  toolUsage[tokenUsage.node].completionTokens += tokenUsage.completionTokens || 0;\n  toolUsage[tokenUsage.node].promptTokens += tokenUsage.promptTokens || 0;\n  toolUsage[tokenUsage.node].totalTokens += tokenUsage.totalTokens || 0;\n  toolUsage[tokenUsage.node].count += 1;\n  \n  if (!toolUsage[tokenUsage.node].models[tokenUsage.model]) {\n    toolUsage[tokenUsage.node].models[tokenUsage.model] = {\n      count: 0,\n      totalTokens: 0\n    };\n  }\n  toolUsage[tokenUsage.node].models[tokenUsage.model].count += 1;\n  toolUsage[tokenUsage.node].models[tokenUsage.model].totalTokens += tokenUsage.totalTokens || 0;\n  \n  // Add to our totals\n  tokens.completionTokens += tokenUsage.completionTokens || 0;\n  tokens.promptTokens += tokenUsage.promptTokens || 0;\n  tokens.totalTokens += tokenUsage.totalTokens || 0;\n});\n\n// Calculate cost estimates based on model\nconst costEstimates = {\n  total: 0\n};\n\n// Cost per 1 K tokens  — verified 2025-06-30\nconst modelCosts = {\n  /* ---------- OpenAI ---------- */\n  \"gpt-4.5\":        { prompt: 0.075,  completion: 0.150 }, // research-preview :contentReference[oaicite:0]{index=0}\n  \"gpt-4.1\":        { prompt: 0.002,  completion: 0.008 }, // :contentReference[oaicite:1]{index=1}\n  \"gpt-4o\":         { prompt: 0.005,  completion: 0.020 }, // :contentReference[oaicite:2]{index=2}\n  \"gpt-4o-mini\":    { prompt: 0.0006, completion: 0.0024 },// :contentReference[oaicite:3]{index=3}\n  \"gpt-4\":          { prompt: 0.030,  completion: 0.060 }, // 8 K legacy tier :contentReference[oaicite:4]{index=4}\n  \"gpt-3.5-turbo\":  { prompt: 0.0005, completion: 0.0015 },// :contentReference[oaicite:5]{index=5}\n  \"o3\":             { prompt: 0.002,  completion: 0.008 }, // :contentReference[oaicite:6]{index=6}\n  \"o3-pro\":         { prompt: 0.020,  completion: 0.080 }, // :contentReference[oaicite:7]{index=7}\n  \"o4-mini\":        { prompt: 0.0011, completion: 0.0044 },// :contentReference[oaicite:8]{index=8}\n\n  /* ---------- Anthropic Claude ---------- */\n  \"claude-opus-4\":      { prompt: 0.015,  completion: 0.075 }, // :contentReference[oaicite:9]{index=9}\n  \"claude-sonnet-4\":    { prompt: 0.003,  completion: 0.015 }, // :contentReference[oaicite:10]{index=10}\n  \"claude-3.5-sonnet\":  { prompt: 0.003,  completion: 0.015 }, // same price band\n  \"claude-3.5-haiku\":   { prompt: 0.0008, completion: 0.004 }, // :contentReference[oaicite:11]{index=11}\n  \"claude-3-opus\":      { prompt: 0.015,  completion: 0.075 }, // :contentReference[oaicite:12]{index=12}\n  \"claude-3-sonnet\":    { prompt: 0.003,  completion: 0.015 }, // :contentReference[oaicite:13]{index=13}\n  \"claude-3-haiku\":     { prompt: 0.00025,completion: 0.00125},// :contentReference[oaicite:14]{index=14}\n\n  /* ---------- Google Gemini ---------- */\n  \"gemini-2.5-pro\":        { prompt: 0.00125, completion: 0.010  }, // :contentReference[oaicite:15]{index=15}\n  \"gemini-2.5-flash\":      { prompt: 0.0003,  completion: 0.0025 }, // :contentReference[oaicite:16]{index=16}\n  \"gemini-2.5-flash-lite\": { prompt: 0.0001,  completion: 0.0004 }, // :contentReference[oaicite:17]{index=17}\n  \"gemini-2.0-flash\":      { prompt: 0.0001,  completion: 0.0004 }, // :contentReference[oaicite:18]{index=18}\n  \"gemini-1.5-pro\":        { prompt: 0.00125, completion: 0.005  }, // :contentReference[oaicite:19]{index=19}\n  \"gemini-1.5-flash\":      { prompt: 0.000075,completion: 0.0003 }, // :contentReference[oaicite:20]{index=20}\n\n  /* ---------- DeepSeek ---------- */\n  \"deepseek-r1\":   { prompt: 0.00055, completion: 0.00219 }, // deepseek-reasoner :contentReference[oaicite:21]{index=21}\n  \"deepseek-v3\":   { prompt: 0.00027, completion: 0.00110 }, // mapped via chat → v3  :contentReference[oaicite:22]{index=22}\n  \"deepseek-chat\": { prompt: 0.00027, completion: 0.00110 }, // :contentReference[oaicite:23]{index=23}\n\n  /* Fallback */\n  \"default\": { prompt: 0.001, completion: 0.003 }\n};\n\n\n// Calculate costs for each model\nObject.entries(modelUsage).forEach(([modelName, usage]) => {\n  const costRates = modelCosts[modelName] || modelCosts.default;\n  \n  const modelCost = {\n    prompt: (usage.promptTokens / 1000) * costRates.prompt,\n    completion: (usage.completionTokens / 1000) * costRates.completion\n  };\n  \n  modelCost.total = modelCost.prompt + modelCost.completion;\n  costEstimates[modelName] = modelCost;\n  costEstimates.total += modelCost.total;\n});\n\n// Calculate costs for each tool/node\nconst toolCostEstimates = {};\nObject.entries(toolUsage).forEach(([toolName, usage]) => {\n  let toolCost = 0;\n  \n  // Calculate cost based on the models used with this tool\n  Object.entries(usage.models || {}).forEach(([modelName, modelUsage]) => {\n    const costRates = modelCosts[modelName] || modelCosts.default;\n    // Use an approximation based on the overall ratio for prompt/completion split\n    const promptRatio = tokens.promptTokens > 0 ? tokens.promptTokens / tokens.totalTokens : 0.5;\n    const completionRatio = tokens.completionTokens > 0 ? tokens.completionTokens / tokens.totalTokens : 0.5;\n    \n    const promptTokens = modelUsage.totalTokens * promptRatio;\n    const completionTokens = modelUsage.totalTokens * completionRatio;\n    \n    const cost = \n      ((promptTokens / 1000) * costRates.prompt) + \n      ((completionTokens / 1000) * costRates.completion);\n    \n    toolCost += cost;\n  });\n  \n  toolCostEstimates[toolName] = toolCost;\n});\n\n// Log summary information\nif (DEBUG) {\n  console.log(\"\\n=== FINAL SUMMARY ===\");\n  console.log(`Total Token Usage Sources Found: ${tokenUsageSources.length}`);\n  console.log(`Total Completion Tokens: ${tokens.completionTokens}`);\n  console.log(`Total Prompt Tokens: ${tokens.promptTokens}`);\n  console.log(`Total Tokens: ${tokens.totalTokens}`);\n  \n  console.log(`\\n--- Excluded Nodes ---`);\n  console.log(`The following node types are excluded from token counting:`);\n  EXCLUDED_NODES.forEach(nodeName => {\n    console.log(`  - ${nodeName}`);\n  });\n  \n  console.log(\"\\n--- Usage by Model ---\");\n  Object.entries(modelUsage).forEach(([model, usage]) => {\n    console.log(`\\n${model}:`);\n    console.log(`  API Calls: ${usage.count}`);\n    console.log(`  Total Tokens: ${usage.totalTokens} (Completion: ${usage.completionTokens}, Prompt: ${usage.promptTokens})`);\n    \n    if (costEstimates[model]) {\n      console.log(`  Estimated Cost: ${costEstimates[model].total.toFixed(6)}`);\n    }\n    \n    if (Object.keys(usage.nodes || {}).length > 0) {\n      console.log(`  Nodes Used:`);\n      Object.entries(usage.nodes).forEach(([node, nodeData]) => {\n        console.log(`    ${node}: ${nodeData.count} times, ${nodeData.totalTokens} tokens`);\n      });\n    }\n  });\n  \n  console.log(\"\\n--- Usage by Node (Only AI/LLM Nodes) ---\");\n  Object.entries(toolUsage).forEach(([tool, usage]) => {\n    console.log(`\\n${tool}:`);\n    console.log(`  API Calls: ${usage.count}`);\n    console.log(`  Total Tokens: ${usage.totalTokens} (Completion: ${usage.completionTokens}, Prompt: ${usage.promptTokens})`);\n    console.log(`  Estimated Cost: ${toolCostEstimates[tool].toFixed(6)}`);\n  });\n  \n  console.log(`\\n--- Total Estimated Cost ---`);\n  console.log(`${costEstimates.total.toFixed(6)}`);\n}\n\n// Prepare the result\nconst result = {\n  ...tokens,\n  timestamp: new Date().toISOString(),\n  source: 'n8n token usage extractor - fixed version with node exclusion',\n  modelsUsed: Array.from(modelsUsed),\n  nodesUsed: Array.from(toolsUsed),\n  excludedNodes: EXCLUDED_NODES,\n  modelUsage,\n  nodeUsage: toolUsage,\n  costEstimate: parseFloat(costEstimates.total.toFixed(6)),\n  modelCosts: Object.fromEntries(\n    Object.entries(costEstimates)\n      .filter(([key]) => key !== 'total')\n      .map(([model, cost]) => [model, parseFloat(cost.total.toFixed(6))])\n  ),\n  nodeCosts: Object.fromEntries(\n    Object.entries(toolCostEstimates)\n      .map(([tool, cost]) => [tool, parseFloat(cost.toFixed(6))])\n  ),\n  details: tokenUsageSources\n};\n\n// Return the result\nreturn [{ json: result }];"},"type":"n8n-nodes-base.code","typeVersion":2,"position":[1760,740],"id":"55e6ec05-aeb5-4c28-9a49-0cc9506e9bfa","name":"Get AI Usage Data1"},{"parameters":{"assignments":{"assignments":[{"id":"04791c5f-747a-40e8-8b70-5b4d798e5651","name":"Ai_Run_Data","value":"={{ $json }}","type":"object"},{"id":"c1b3dd24-5caf-43ac-9d86-bf8eadb21aac","name":"modelsUsed","value":"={{ $json.modelsUsed }}","type":"array"},{"id":"ac88eb78-f572-47a2-a6c0-0588c113ba76","name":"executionId","value":"={{ $('Webhook2').item.json.body.executionId }}","type":"string"},{"id":"13a5d364-8744-4bad-8076-af44e3b8ce42","name":"workflowId","value":"={{ $('Webhook2').item.json.body.workflowId }}","type":"string"},{"id":"5607d76a-6685-4993-b590-2b350d5af6ab","name":"workflowName","value":"={{ $('Webhook2').item.json.body.workflowName }}","type":"string"},{"id":"e5328a81-5877-45be-abaf-ea37440027e2","name":"link","value":"={{ $execution.resumeUrl.split('/').slice(0, 3).join('/') }}/workflow/{{ $('Webhook2').item.json.body.workflowId }}/executions/{{ $('Webhook2').item.json.body.executionId }}","type":"string"}]},"options":{}},"type":"n8n-nodes-base.set","typeVersion":3.4,"position":[2040,740],"id":"0ab3b17c-167b-4718-aaab-1b8f3a4be128","name":"Set Ai_Run_Data1"},{"parameters":{"url":"=https://r.jina.ai/https://www.helicone.ai/llm-cost\n","authentication":"genericCredentialType","genericAuthType":"httpHeaderAuth","options":{}},"id":"c888a70b-2a18-4650-8b4c-2d6ef3b38c98","name":"Get AI Pricing1","type":"n8n-nodes-base.httpRequest","typeVersion":4.2,"position":[2320,740],"retryOnFail":true,"credentials":{"httpHeaderAuth":{"id":"DFida4ko9mtk5ObN","name":"Header Auth account"}},"onError":"continueRegularOutput"},{"parameters":{"jsCode":"// CostCalcFromJina.js\n// This script extracts pricing for models and calculates costs in one step\n\n// Get the markdown data and AI run data from the input\nconst markdownData = $input.first().json.data;\nconst aiRunData = $('Set Ai_Run_Data1').first().json.Ai_Run_Data;\nconst modelsUsed = aiRunData.modelsUsed || [];\n\n// Initialize the result object\nconst result = {\n  originalData: aiRunData,\n  modelPricing: {},\n  updatedCosts: {},\n  summary: {\n    totalCost: 0,\n    byModel: {},\n    byTool: {}\n  },\n  timestamp: new Date().toISOString()\n};\n\n// Function to format cost as USD\nfunction formatUSD(amount) {\n  return parseFloat(amount).toFixed(4);\n}\n\n// Function to parse currency strings to numbers\nfunction parseCurrency(currencyStr) {\n  if (!currencyStr) return 0;\n  \n  // Remove the $ sign and convert to number\n  return parseFloat(currencyStr.replace('$', ''));\n}\n\n// Function to normalize model names\nfunction normalizeModelName(modelName) {\n  return modelName.toLowerCase().trim();\n}\n\n// Extract pricing table from markdown\nfunction extractPricingFromMarkdown(markdown) {\n  // Find the table in the markdown\n  const tableRegex = /\\|\\s*Provider\\s*\\|\\s*Model\\s*\\|\\s*Input\\/1k\\s*\\nTokens\\s*\\|\\s*Output\\/1k\\s*\\nTokens\\s*\\|[\\s\\S]*?(?=\\n\\n|\\n$|$)/;\n  const tableMatch = markdown.match(tableRegex);\n  \n  if (!tableMatch) {\n    return [];\n  }\n  \n  const tableContent = tableMatch[0];\n  \n  // Split the table into rows\n  const rows = tableContent.split('\\n');\n  \n  // Skip the header and separator rows\n  const dataRows = rows.slice(2);\n  \n  // Parse each row\n  return dataRows.map(row => {\n    // Split the row by the pipe character\n    const cells = row.split('|').map(cell => cell.trim()).filter(cell => cell);\n    \n    // Skip if we don't have enough cells\n    if (cells.length < 7) return null;\n    \n    // Extract the relevant information\n    // Format: Provider | Model | Input/1k Tokens | Output/1k Tokens | Input Cost | Output Cost | Total Cost\n    const provider = cells[0].replace(/\\[|\\]/g, '').trim();\n    \n    // Extract model name from markdown link if present\n    let modelName = cells[1];\n    const modelMatch = modelName.match(/\\[(.*?)\\]/);\n    if (modelMatch) {\n      modelName = modelMatch[1];\n    }\n    \n    // Extract input and output costs per 1k tokens\n    let inputCostPer1k = cells[2];\n    let outputCostPer1k = cells[3];\n    \n    // Extract costs from markdown links if present\n    const inputCostMatch = inputCostPer1k.match(/\\[\\$(.*?)\\]/);\n    if (inputCostMatch) {\n      inputCostPer1k = inputCostMatch[1];\n    }\n    \n    const outputCostMatch = outputCostPer1k.match(/\\[\\$(.*?)\\]/);\n    if (outputCostMatch) {\n      outputCostPer1k = outputCostMatch[1];\n    }\n    \n    // Convert costs to numbers\n    const inputCost = parseCurrency(inputCostPer1k);\n    const outputCost = parseCurrency(outputCostPer1k);\n    \n    return {\n      provider,\n      model: modelName,\n      inputCostPer1k: inputCost,\n      outputCostPer1k: outputCost\n    };\n  }).filter(Boolean); // Remove any null entries\n}\n\n// Extract pricing information\nconst pricingData = extractPricingFromMarkdown(markdownData);\n\n// Create a map of model names to pricing\nconst modelPriceMap = {};\npricingData.forEach(item => {\n  const normalizedName = normalizeModelName(item.model);\n  modelPriceMap[normalizedName] = {\n    provider: item.provider,\n    model: item.model,\n    inputCostPer1k: item.inputCostPer1k,\n    outputCostPer1k: item.outputCostPer1k\n  };\n});\n\n// Function to find pricing for a specific model\nfunction findModelPricing(modelName) {\n  const normalizedName = normalizeModelName(modelName);\n  \n  // Try exact match first\n  if (modelPriceMap[normalizedName]) {\n    return modelPriceMap[normalizedName];\n  }\n  \n  // Try partial match\n  const matchingKeys = Object.keys(modelPriceMap).filter(key => \n    normalizedName.includes(key) || key.includes(normalizedName)\n  );\n  \n  if (matchingKeys.length > 0) {\n    // Sort by length to get the closest match (longer matches are likely more specific)\n    matchingKeys.sort((a, b) => b.length - a.length);\n    return modelPriceMap[matchingKeys[0]];\n  }\n  \n  // Handle special cases\n  if (normalizedName.includes('gpt-4')) {\n    if (normalizedName.includes('o-')) {\n      // GPT-4o models\n      return {\n        provider: 'OpenAI',\n        model: 'gpt-4o',\n        inputCostPer1k: 0.0025,\n        outputCostPer1k: 0.01\n      };\n    } else {\n      // Regular GPT-4 models\n      return {\n        provider: 'OpenAI',\n        model: 'gpt-4',\n        inputCostPer1k: 0.03,\n        outputCostPer1k: 0.06\n      };\n    }\n  }\n  \n  if (normalizedName.includes('gpt-3.5')) {\n    return {\n      provider: 'OpenAI',\n      model: 'gpt-3.5-turbo',\n      inputCostPer1k: 0.0005,\n      outputCostPer1k: 0.0015\n    };\n  }\n  \n  // Default pricing if no match found\n  return {\n    provider: 'Unknown',\n    model: modelName,\n    inputCostPer1k: 0.01,\n    outputCostPer1k: 0.03,\n    isDefault: true\n  };\n}\n\n// Process each model used\nmodelsUsed.forEach(modelName => {\n  // Get the pricing data for this model\n  const pricingData = findModelPricing(modelName);\n  \n  // Store the pricing data\n  result.modelPricing[modelName] = pricingData;\n  \n  // Get the model usage data\n  const modelUsage = aiRunData.modelUsage[modelName];\n  \n  if (!modelUsage) return; // Skip if no usage data for this model\n  \n  // Calculate accurate costs\n  const promptCost = (modelUsage.promptTokens / 1000) * pricingData.inputCostPer1k;\n  const completionCost = (modelUsage.completionTokens / 1000) * pricingData.outputCostPer1k;\n  const totalCost = promptCost + completionCost;\n  \n  // Store the updated costs\n  result.updatedCosts[modelName] = {\n    promptCost: formatUSD(promptCost),\n    completionCost: formatUSD(completionCost),\n    totalCost: formatUSD(totalCost),\n    pricingUsed: {\n      inputCostPer1k: pricingData.inputCostPer1k,\n      outputCostPer1k: pricingData.outputCostPer1k\n    }\n  };\n  \n  // Add to summary\n  result.summary.byModel[modelName] = formatUSD(totalCost);\n  result.summary.totalCost += totalCost;\n  \n  // Calculate costs by tool for this model\n  if (modelUsage.tools) {\n    Object.entries(modelUsage.tools).forEach(([toolName, toolData]) => {\n      // Estimate the prompt/completion split for this tool based on the overall model ratio\n      const promptRatio = modelUsage.promptTokens / modelUsage.totalTokens;\n      const completionRatio = modelUsage.completionTokens / modelUsage.totalTokens;\n      \n      const toolPromptTokens = toolData.totalTokens * promptRatio;\n      const toolCompletionTokens = toolData.totalTokens * completionRatio;\n      \n      const toolPromptCost = (toolPromptTokens / 1000) * pricingData.inputCostPer1k;\n      const toolCompletionCost = (toolCompletionTokens / 1000) * pricingData.outputCostPer1k;\n      const toolTotalCost = toolPromptCost + toolCompletionCost;\n      \n      // Add to the tool summary\n      if (!result.summary.byTool[toolName]) {\n        result.summary.byTool[toolName] = {\n          totalCost: 0,\n          byModel: {}\n        };\n      }\n      \n      result.summary.byTool[toolName].totalCost += toolTotalCost;\n      result.summary.byTool[toolName].byModel[modelName] = formatUSD(toolTotalCost);\n    });\n  }\n});\n\n// Format the total cost in the summary\nresult.summary.totalCost = formatUSD(result.summary.totalCost);\n\n// Format tool costs\nObject.keys(result.summary.byTool).forEach(toolName => {\n  result.summary.byTool[toolName].totalCost = formatUSD(result.summary.byTool[toolName].totalCost);\n});\n\n// Return the result\nreturn [{\n  json: {\n    ...result\n  }\n}];"},"type":"n8n-nodes-base.code","typeVersion":2,"position":[2600,740],"id":"3f6b8ec0-0d65-4deb-b485-577d3bb86a56","name":"Get Models Price and Add Summary1"},{"parameters":{"method":"POST","url":"={{ $execution.resumeUrl.split('/').slice(0, 3).join('/') }}/webhook/token-estim8r-data","sendBody":true,"bodyParameters":{"parameters":[{"name":"executionId","value":"={{$execution.id}}"},{"name":"workflowId","value":"={{$workflow.id}}"},{"name":"workflowName","value":"={{$workflow.name}}"}]},"options":{}},"type":"n8n-nodes-base.httpRequest","typeVersion":4.2,"position":[960,140],"id":"cafba4c4-3b01-48c6-9012-2fcb225fe242","name":"Send Token Estim8r Data"},{"parameters":{"content":"# 🔑 Copy This node to the end of your workflow.  👇","height":380,"width":600,"color":3},"type":"n8n-nodes-base.stickyNote","position":[720,0],"typeVersion":1,"id":"46f0520e-d6bc-46ef-8daa-69b729a0e2d3","name":"Sticky Note2"},{"parameters":{"operation":"append","documentId":{"__rl":true,"value":"1jblHec5ufkwfbtbVvTkizb2QNjP_guh06bclHpErorw","mode":"list","cachedResultName":"20250630_token estimater","cachedResultUrl":"https://docs.google.com/spreadsheets/d/1jblHec5ufkwfbtbVvTkizb2QNjP_guh06bclHpErorw/edit?usp=drivesdk"},"sheetName":{"__rl":true,"value":553300772,"mode":"list","cachedResultName":"csv template","cachedResultUrl":"https://docs.google.com/spreadsheets/d/1jblHec5ufkwfbtbVvTkizb2QNjP_guh06bclHpErorw/edit#gid=553300772"},"columns":{"mappingMode":"defineBelow","value":{"timestamp":"={{ $json.originalData.timestamp }}","Completion Tokens":"={{ $json.originalData.completionTokens }}","Prompt Tokens":"={{ $json.originalData.promptTokens }}","Models Used":"={{ $json.originalData.modelsUsed.join(', ') }}","Tools Used":"={{ $json.originalData.toolsUsed.join(', ').replaceAll(\"subRun,\",\"\").replaceAll(\" node,\",\"\").replaceAll(\" runIndex,\",\"\").replaceAll(\" key,\",\"\").replaceAll(\", value\",\"\").replaceAll(\" metadataProperties,\",\"\") }}","Total Cost":"={{ $json.summary.totalCost }}","Json Array":"={{ JSON.stringify($json.originalData) }}","Workflow Name":"={{ $('Set Ai_Run_Data1').item.json.workflowName }}","Workflow ID":"={{ $('Set Ai_Run_Data1').item.json.workflowId }}","Workflow Execution Id":"={{ $('Set Ai_Run_Data1').item.json.executionId }}","Total Tokens":"={{ $json.originalData.totalTokens }}"},"matchingColumns":[],"schema":[{"id":"timestamp","displayName":"timestamp","required":false,"defaultMatch":false,"display":true,"type":"string","canBeUsedToMatch":true,"removed":false},{"id":"Workflow Name","displayName":"Workflow Name","required":false,"defaultMatch":false,"display":true,"type":"string","canBeUsedToMatch":true,"removed":false},{"id":"Workflow ID","displayName":"Workflow ID","required":false,"defaultMatch":false,"display":true,"type":"string","canBeUsedToMatch":true,"removed":false},{"id":"Workflow Execution Id","displayName":"Workflow Execution Id","required":false,"defaultMatch":false,"display":true,"type":"string","canBeUsedToMatch":true,"removed":false},{"id":"Total Tokens","displayName":"Total Tokens","required":false,"defaultMatch":false,"display":true,"type":"string","canBeUsedToMatch":true,"removed":false},{"id":"Prompt Tokens","displayName":"Prompt Tokens","required":false,"defaultMatch":false,"display":true,"type":"string","canBeUsedToMatch":true,"removed":false},{"id":"Completion Tokens","displayName":"Completion Tokens","required":false,"defaultMatch":false,"display":true,"type":"string","canBeUsedToMatch":true,"removed":false},{"id":"Models Used","displayName":"Models Used","required":false,"defaultMatch":false,"display":true,"type":"string","canBeUsedToMatch":true,"removed":false},{"id":"Tools Used","displayName":"Tools Used","required":false,"defaultMatch":false,"display":true,"type":"string","canBeUsedToMatch":true,"removed":false},{"id":"Total Cost","displayName":"Total Cost","required":false,"defaultMatch":false,"display":true,"type":"string","canBeUsedToMatch":true,"removed":false},{"id":"Json Array","displayName":"Json Array","required":false,"defaultMatch":false,"display":true,"type":"string","canBeUsedToMatch":true,"removed":false}],"attemptToConvertTypes":false,"convertFieldsToString":false},"options":{}},"type":"n8n-nodes-base.googleSheets","typeVersion":4.5,"position":[2900,740],"id":"a6b0a3db-9ef5-4953-9841-85d5249487b7","name":"Google Sheets - Add Usage","credentials":{"googleSheetsOAuth2Api":{"id":"2r0S3wqaeXCs9rgr","name":"Google Sheets account"}}}],"connections":{"When clicking ‘Test workflow’":{"main":[[{"node":"Wait","type":"main","index":0}]]},"Wait":{"main":[[{"node":"n8n - Get Execution","type":"main","index":0}]]},"Webhook2":{"main":[[{"node":"Wait","type":"main","index":0}]]},"n8n - Get Execution":{"main":[[{"node":"Get AI Usage Data1","type":"main","index":0}]]},"Get AI Usage Data1":{"main":[[{"node":"Set Ai_Run_Data1","type":"main","index":0}]]},"Set Ai_Run_Data1":{"main":[[{"node":"Get AI Pricing1","type":"main","index":0}]]},"Get Models Price and Add Summary1":{"main":[[{"node":"Google Sheets - Add Usage","type":"main","index":0}]]},"Get AI Pricing1":{"main":[[{"node":"Get Models Price and Add Summary1","type":"main","index":0}]]}},"settings":{"executionOrder":"v1"},"staticData":null,"meta":{"templateCredsSetupCompleted":true},"pinData":{},"versionId":"7da172c0-f831-454d-99dd-d7b27df24dab","triggerCount":1,"tags":[{"createdAt":"2025-06-18T07:58:49.669Z","updatedAt":"2025-07-03T09:01:04.709Z","id":"w3Cnd8StpKPr7ekR","name":"on test"}]}